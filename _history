{"entries":[{"timestamp":1748648312164,"editorVersion":"2.0.56","changes":[{"type":"edited","filename":"main.blocks","patch":[{"start1":0,"length1":131,"diffs":[[1,"<xml xmlns=\"http://www.w3.org/1999/xhtml\">\n  <variables></variables>\n  <block type=\"pxt-on-start\" x=\"0\" y=\"0\"></block>\n</xml>"]]}]},{"type":"edited","filename":"main.ts","patch":[{"start1":0,"length1":1,"diffs":[[1," "]]}]},{"type":"edited","filename":"pxt.json","patch":[{"start1":196,"length1":61,"diffs":[[1,"        \"assets.json\"\n"]]},{"start1":225,"length1":31,"diffs":[[1,"    \"additionalFilePaths\": []\n"]]}]},{"type":"added","filename":"main.py","value":""},{"type":"added","filename":"torch.ts","value":"// torch.ts - Neural Network Library with Explicit Neurons\n\n// Tensor Class (Basic Matrix Operations)\nclass Tensor {\n    data: number[][];\n\n    constructor(data: number[][]) {\n        this.data = data;\n    }\n\n    matmul(other: Tensor): Tensor | null {\n        let rowsA = this.data.length;\n        let colsA = this.data[0].length;\n        let rowsB = other.data.length;\n        let colsB = other.data[0].length;\n\n        if (colsA !== rowsB) {\n            return null; // Dimension mismatch\n        }\n\n        let result: number[][] = [];\n        for (let r = 0; r < rowsA; r++) {\n            let row: number[] = [];\n            for (let c = 0; c < colsB; c++) {\n                let total = 0;\n                for (let i = 0; i < colsA; i++) {\n                    total += this.data[r][i] * other.data[i][c];\n                }\n                row.push(total);\n            }\n            result.push(row);\n        }\n        return new Tensor(result);\n    }\n\n    applyFunction(func: (x: number) => number): Tensor {\n        return new Tensor(this.data.map(row => row.map(func)));\n    }\n\n    add(other: Tensor): Tensor {\n        return new Tensor(this.data.map((row, r) => row.map((val, c) => val + other.data[r][c])));\n    }\n}\n\n// **Neuron Class (Handles Individual Weights)**\nclass Neuron {\n    weights: number[];\n    bias: number;\n\n    constructor(inputSize: number) {\n        this.weights = [];\n        this.bias = Math.random() * 0.2 - 0.1;\n\n        for (let i = 0; i < inputSize; i++) {\n            this.weights.push(Math.random() * 0.2 - 0.1);\n        }\n    }\n\n    activate(inputs: number[]): number {\n        let sum = inputs.reduce((acc, val, i) => acc + val * this.weights[i], this.bias);\n        return Math.max(0, sum); // Using ReLU for activation\n    }\n}\n\n// **Neural Network Layer (Uses Multiple Neurons)**\nclass Linear {\n    neurons: Neuron[];\n\n    constructor(inDim: number, outDim: number) {\n        this.neurons = [];\n        for (let i = 0; i < outDim; i++) {\n            this.neurons.push(new Neuron(inDim));\n        }\n    }\n\n    forward(input: Tensor): Tensor {\n        let output: number[][] = [];\n\n        for (let row of input.data) {\n            let neuronOutputs: number[] = this.neurons.map(neuron => neuron.activate(row));\n            output.push(neuronOutputs);\n        }\n\n        return new Tensor(output);\n    }\n}\n\n// **Activation functions**\nfunction relu(tensor: Tensor): Tensor {\n    return tensor.applyFunction(x => Math.max(0, x));\n}\n\n// **Example usage**\nlet x = new Tensor([[0.5, -0.2]]);\nlet layer = new Linear(2, 3);\nlet output = layer.forward(x);\n\nconsole.log(JSON.stringify(output.data));"}]},{"timestamp":1748648847534,"editorVersion":"2.0.56","changes":[{"type":"edited","filename":"torch.ts","patch":[{"start1":0,"length1":66,"diffs":[[1,"// torch.ts - Neural Network Library with Explicit Neurons\n"]]},{"start1":60,"length1":70,"diffs":[[1,"// Tensor Class (Basic Matrix Operations)\nclass Tensor {\n    data: number[][];\n"]]},{"start1":140,"length1":80,"diffs":[[1,"    constructor(data: number[][]) {\n        this.data = data;\n    }\n"]]},{"start1":209,"length1":223,"diffs":[[1,"    matmul(other: Tensor): Tensor | null {\n        let rowsA = this.data.length;\n        let colsA = this.data[0].length;\n        let rowsB = other.data.length;\n        let colsB = other.data[0].length;\n"]]},{"start1":413,"length1":100,"diffs":[[1,"        if (colsA !== rowsB) {\n            return null; // Dimension mismatch\n        }\n"]]},{"start1":502,"length1":394,"diffs":[[1,"        let result: number[][] = [];\n        for (let r = 0; r < rowsA; r++) {\n            let row: number[] = [];\n            for (let c = 0; c < colsB; c++) {\n                let total = 0;\n                for (let i = 0; i < colsA; i++) {\n                    total += this.data[r][i] * other.data[i][c];\n"]]},{"start1":827,"length1":34,"diffs":[[1,"                row.push(total);\n"]]},{"start1":874,"length1":39,"diffs":[[1,"            result.push(row);\n"]]},{"start1":914,"length1":0,"diffs":[[1,"        return new Tensor(result);\n    }\n"]]},{"start1":956,"length1":139,"diffs":[[1,"    applyFunction(func: (x: number) => number): Tensor {\n        return new Tensor(this.data.map(row => row.map(func)));\n    }\n"]]},{"start1":1084,"length1":150,"diffs":[[1,"    add(other: Tensor): Tensor {\n        return new Tensor(this.data.map((row, r) => row.map((val, c) => val + other.data[r][c])));\n"]]},{"start1":1222,"length1":0,"diffs":[[1,"}\n"]]},{"start1":1225,"length1":75,"diffs":[[1,"// **Neuron Class (Handles Individual Weights)**\nclass Neuron {\n    weights: number[];\n    bias: number;\n"]]},{"start1":1331,"length1":123,"diffs":[[1,"    constructor(inputSize: number) {\n        this.weights = [];\n        this.bias = Math.random() * 0.2 - 0.1;\n"]]},{"start1":1443,"length1":126,"diffs":[[1,"        for (let i = 0; i < inputSize; i++) {\n            this.weights.push(Math.random() * 0.2 - 0.1);\n"]]},{"start1":1557,"length1":0,"diffs":[[1,"    }\n"]]},{"start1":1564,"length1":220,"diffs":[[1,"    activate(inputs: number[]): number {\n        let sum = inputs.reduce((acc, val, i) => acc + val * this.weights[i], this.bias);\n        return Math.max(0, sum); // Using ReLU for activation\n"]]},{"start1":1763,"length1":0,"diffs":[[1,"}\n"]]},{"start1":1766,"length1":53,"diffs":[[1,"// **Neural Network Layer (Uses Multiple Neurons)**\nclass Linear {\n    neurons: Neuron[];\n"]]},{"start1":1857,"length1":199,"diffs":[[1,"    constructor(inDim: number, outDim: number) {\n        this.neurons = [];\n        for (let i = 0; i < outDim; i++) {\n            this.neurons.push(new Neuron(inDim));\n"]]},{"start1":2036,"length1":1410,"diffs":[[1,""]]},{"start1":2043,"length1":53,"diffs":[[1,"    forward(input: Tensor): Tensor {\n        let output: number[][] = [];\n"]]},{"start1":2118,"length1":472,"diffs":[[1,"        for (let row of input.data) {\n            let neuronOutputs: number[] = this.neurons.map(neuron => neuron.activate(row));\n            output.push(neuronOutputs);\n"]]},{"start1":2299,"length1":95,"diffs":[[1,"        return new Tensor(output);\n"]]},{"start1":2340,"length1":0,"diffs":[[1,"}\n"]]},{"start1":2343,"length1":147,"diffs":[[1,"// **Activation functions**\nfunction relu(tensor: Tensor): Tensor {\n    return tensor.applyFunction(x => Math.max(0, x));\n}\n"]]},{"start1":2468,"length1":354,"diffs":[[1,"// **Example usage**\nlet x = new Tensor([[0.5, -0.2]]);\nlet layer = new Linear(2, 3);\nlet output = layer.forward(x);\n"]]},{"start1":2586,"length1":84,"diffs":[[1,"console.log(JSON.stringify(output.data));"]]}]}]},{"timestamp":1748649643180,"editorVersion":"2.0.56","changes":[{"type":"edited","filename":"main.py","patch":[{"start1":0,"length1":1,"diffs":[[1,""]]}]}]},{"timestamp":1748649646109,"editorVersion":"2.0.56","changes":[{"type":"edited","filename":"main.ts","patch":[{"start1":0,"length1":37,"diffs":[[1,""]]},{"start1":1,"length1":793,"diffs":[[1,""]]}]},{"type":"edited","filename":"pxt.json","patch":[{"start1":219,"length1":0,"diffs":[[1,"        \"main.py\",\n"]]}]},{"type":"edited","filename":"torch.ts","patch":[{"start1":544,"length1":53,"diffs":[[1,""]]},{"start1":721,"length1":33,"diffs":[[1,"                    let total = 0;\n                    for (let i = 0; i < colsA; i++) {\n                        total += this.data[r][i] * other.data[i][c];\n                    }\n                    row.push(total);\n"]]},{"start1":1004,"length1":394,"diffs":[[1,""]]},{"start1":1053,"length1":9,"diffs":[[1,"\n"]]},{"start1":1344,"length1":745,"diffs":[[1,""]]},{"start1":2745,"length1":1,"diffs":[[1,""]]},{"start1":2803,"length1":88,"diffs":[[1,"                    let prediction = this.forward(inputs[i], relu);\n                    totalLoss += mae(prediction, targets[i]);\n"]]},{"start1":2934,"length1":199,"diffs":[[1,"                    let error = targets[i].add(prediction.applyFunction(x => -x));\n                    let weightUpdate = inputs[i].matmul(error.applyFunction(x => learningRate * x));\n"]]},{"start1":3119,"length1":447,"diffs":[[1,"                    if (weightUpdate) {\n                        this.neurons.forEach((neuron, index) => {\n                            neuron.weights = neuron.weights.map((w, j) => w + weightUpdate.data[0][j]);\n                            neuron.bias += error.data[0][index] * learningRate;\n                        });\n                    }\n"]]},{"start1":3477,"length1":1,"diffs":[[1,""]]}]},{"type":"removed","filename":"main.py","value":"\n"}]},{"timestamp":1748650243615,"editorVersion":"2.0.56","changes":[{"type":"edited","filename":"main.blocks","patch":[{"start1":0,"length1":84,"diffs":[[1,"<xml xmlns=\"https://developers.google.com/blockly/xml\"><variables></variables><block type=\"pxt-on-start\" x=\"0\" y=\"0\"></block></xml>"]]}]},{"type":"edited","filename":"main.ts","patch":[{"start1":764,"length1":68,"diffs":[[1,"console.log(\"Predicted Output:\" + JSON.stringify(prediction.data);\n"]]}]},{"type":"edited","filename":"pxt.json","patch":[{"start1":245,"length1":44,"diffs":[[1,""]]}]},{"type":"added","filename":"test.ts","value":"// tests go here; this will not be compiled when this package is used as an extension.\n"}]}],"snapshots":[{"timestamp":1748648312163,"editorVersion":"2.0.56","text":{"main.blocks":"<xml xmlns=\"http://www.w3.org/1999/xhtml\">\n  <variables></variables>\n  <block type=\"pxt-on-start\" x=\"0\" y=\"0\"></block>\n</xml>","main.ts":" ","README.md":" ","assets.json":"","pxt.json":"{\n    \"name\": \"torch? or is it impossible\",\n    \"description\": \"\",\n    \"dependencies\": {\n        \"device\": \"*\"\n    },\n    \"files\": [\n        \"main.blocks\",\n        \"main.ts\",\n        \"README.md\",\n        \"assets.json\"\n    ],\n    \"additionalFilePaths\": []\n}\n"}},{"timestamp":1748650115817,"editorVersion":"2.0.56","text":{"main.blocks":"<xml xmlns=\"https://developers.google.com/blockly/xml\"><variables></variables><block type=\"pxt-on-start\" x=\"0\" y=\"0\"></block></xml>","main.ts":"\n","README.md":"# Torch - Neural Network Library for MakeCode Arcade\r\n\r\nTorch is a lightweight neural network library designed for use within MakeCode Arcade. It provides essential tensor operations, basic neurons, and layers for constructing and training small-scale neural networks.\r\n\r\n## Features\r\n- **Tensor Operations**: Matrix multiplication, element-wise function applications, and tensor addition.\r\n- **Neuron & Linear Layers**: Simple neuron model and fully connected layers.\r\n- **Convolutional Layer**: Basic convolutional operations for feature extraction.\r\n- **Training Support**: Mean absolute error (MAE) loss and simple weight updates.\r\n- **Activation Functions**: Includes ReLU for non-linear transformations.\r\n","assets.json":"","torch.ts":"// torch.ts - Extended Neural Network Library for MakeCode Arcade\n\nnamespace Torch {\n    export class Tensor {\n        data: number[][];\n\n        constructor(data: number[][]) {\n            this.data = data;\n        }\n\n        matmul(other: Tensor): Tensor | null {\n            let rowsA = this.data.length;\n            let colsA = this.data[0].length;\n            let rowsB = other.data.length;\n            let colsB = other.data[0].length;\n\n            if (colsA !== rowsB) {\n                return null; // Dimension mismatch\n            }\n\n            // Manually initialize the result matrix\n            let result: number[][] = [];\n            for (let r = 0; r < rowsA; r++) {\n                let row: number[] = [];\n                for (let c = 0; c < colsB; c++) {\n                    row.push(0);\n                }\n                result.push(row);\n            }\n\n            // Optimized matrix multiplication\n            for (let r = 0; r < rowsA; r++) {\n                for (let i = 0; i < colsA; i++) {\n                    let value = this.data[r][i]; // Reduce lookup overhead\n                    for (let c = 0; c < colsB; c++) {\n                        result[r][c] += value * other.data[i][c];\n                    }\n                }\n            }\n\n            return new Tensor(result);\n        }\n        \n        applyFunction(func: (x: number) => number): Tensor {\n            return new Tensor(this.data.map(row => row.map(func)));\n        }\n\n        add(other: Tensor): Tensor {\n            return new Tensor(this.data.map((row, r) => row.map((val, c) => val + other.data[r][c])));\n        }\n    }\n\n    export class Neuron {\n        weights: number[];\n        bias: number;\n\n        constructor(inputSize: number) {\n            this.weights = [];\n            this.bias = Math.random() * 0.2 - 0.1;\n\n            for (let i = 0; i < inputSize; i++) {\n                this.weights.push(Math.random() * 0.2 - 0.1);\n            }\n        }\n\n        activate(inputs: number[], activation: (x: number) => number): number {\n            let sum = inputs.reduce((acc, val, i) => acc + val * this.weights[i], this.bias);\n            return activation(sum);\n        }\n    }\n\n    export class Linear {\n        neurons: Neuron[];\n\n        constructor(inDim: number, outDim: number) {\n            this.neurons = [];\n            for (let i = 0; i < outDim; i++) {\n                this.neurons.push(new Neuron(inDim));\n            }\n        }\n\n        forward(input: Tensor, activation: (x: number) => number): Tensor {\n            let output: number[][] = [];\n            for (let row of input.data) {\n                let neuronOutputs: number[] = this.neurons.map(neuron => neuron.activate(row, activation));\n                output.push(neuronOutputs);\n            }\n            return new Tensor(output);\n        }\n\n        train(inputs: Tensor[], targets: Tensor[], learningRate: number, epochs: number): void {\n            for (let epoch = 0; epoch < epochs; epoch++) {\n                let totalLoss = 0;\n                for (let i = 0; i < inputs.length; i++) {\n                    let prediction = this.forward(inputs[i], relu);\n                    totalLoss += mae(prediction, targets[i]);\n\n                    let error = targets[i].add(prediction.applyFunction(x => -x));\n                    let weightUpdate = inputs[i].matmul(error.applyFunction(x => learningRate * x));\n\n                    if (weightUpdate) {\n                        this.neurons.forEach((neuron, index) => {\n                            neuron.weights = neuron.weights.map((w, j) => w + weightUpdate.data[0][j]);\n                            neuron.bias += error.data[0][index] * learningRate;\n                        });\n                    }\n                }\n                console.log(`Epoch ${epoch + 1}, Loss: ${totalLoss / inputs.length}`);\n            }\n        }\n    }\n\n    export class ConvLayer {\n        kernel: Tensor;\n\n        constructor(size: number) {\n            // **Fix unsupported `.fill().map()` usage with explicit loops**\n            let kernelArray: number[][] = [];\n            for (let i = 0; i < size; i++) {\n                let row: number[] = [];\n                for (let j = 0; j < size; j++) {\n                    row.push(Math.random() * 0.2 - 0.1);\n                }\n                kernelArray.push(row);\n            }\n            this.kernel = new Tensor(kernelArray);\n        }\n\n        apply(input: Tensor): Tensor {\n            return input.matmul(this.kernel);\n        }\n    }\n\n    export function mae(predictions: Tensor, targets: Tensor): number {\n        let errorTensor = predictions.add(targets.applyFunction(x => -x));\n\n        // **Fix unsupported `.flat()` method by manually summing**\n        let totalError = 0;\n        let elementCount = 0;\n        for (let row of errorTensor.data) {\n            for (let val of row) {\n                totalError += Math.abs(val);\n                elementCount++;\n            }\n        }\n        return totalError / elementCount;\n    }\n\n    export function relu(x: number): number {\n        return Math.max(0, x);\n    }\n}","pxt.json":"{\n    \"name\": \"torch? or is it impossible\",\n    \"description\": \"\",\n    \"dependencies\": {\n        \"device\": \"*\"\n    },\n    \"files\": [\n        \"main.blocks\",\n        \"main.ts\",\n        \"README.md\",\n        \"assets.json\",\n        \"torch.ts\"\n    ],\n    \"preferredEditor\": \"tsprj\"\n}\n"}}],"shares":[],"lastSaveTime":1748650310175}